#!/usr/bin/env python3
"""
Script para an√°lise dos resultados dos testes de performance do Spring PetClinic

Este script processa os arquivos CSV gerados pelo Locust e gera:
- Estat√≠sticas consolidadas por cen√°rio
- Gr√°ficos comparativos
- Relat√≥rio final em formato texto

Requer: pandas, matplotlib, seaborn
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import glob
from pathlib import Path
import numpy as np
from datetime import datetime

def load_results_data(results_dir="results"):
    """Carrega todos os arquivos CSV de resultados"""
    scenarios = {"CenarioA": [], "CenarioB": [], "CenarioC": []}
    
    for scenario in scenarios.keys():
        # Buscar na estrutura: results/CenarioX/repY/results_stats.csv
        pattern = f"{results_dir}/{scenario}/rep*/results_stats.csv"
        files = glob.glob(pattern)
        
        print(f"Procurando arquivos para {scenario}: {pattern}")
        print(f"Encontrados {len(files)} arquivos")
        
        for file in files:
            try:
                df = pd.read_csv(file)
                # Filtrar apenas requisi√ß√µes individuais (n√£o agregadas)
                df_filtered = df[df['Name'] != 'Aggregated'].copy()
                df_filtered['scenario'] = scenario
                df_filtered['file_path'] = file
                scenarios[scenario].append(df_filtered)
                print(f"  Carregado: {file} ({len(df_filtered)} linhas)")
            except Exception as e:
                print(f"Erro ao ler {file}: {e}")
    
    return scenarios

def calculate_scenario_stats(dfs):
    """Calcula estat√≠sticas consolidadas para um cen√°rio"""
    if not dfs:
        return None
    
    # Concatenar todos os DataFrames do cen√°rio
    combined = pd.concat(dfs, ignore_index=True)
    
    # Calcular estat√≠sticas agregadas por repeti√ß√£o
    stats = {
        'avg_response_time': combined['Average Response Time'].mean(),
        'max_response_time': combined['Max Response Time'].max(),
        'min_response_time': combined['Min Response Time'].min(),
        'median_response_time': combined['Median Response Time'].mean(),
        'avg_requests_per_sec': combined['Requests/s'].sum() / len(dfs),  # Soma por repeti√ß√£o, depois m√©dia
        'total_requests': combined['Request Count'].sum(),
        'failure_count': combined['Failure Count'].sum(),
        'success_rate': ((combined['Request Count'].sum() - combined['Failure Count'].sum()) / 
                        combined['Request Count'].sum() * 100) if combined['Request Count'].sum() > 0 else 0,
        'repetitions': len(dfs),
        'p95_response_time': combined['95%'].mean(),
        'p99_response_time': combined['99%'].mean()
    }
    
    return stats

def create_comparison_table(scenario_stats):
    """Cria tabela comparativa dos cen√°rios"""
    df_comparison = pd.DataFrame(scenario_stats).T
    
    # Renomear colunas para portugu√™s
    column_mapping = {
        'avg_response_time': 'Tempo M√©dio (ms)',
        'max_response_time': 'Tempo M√°ximo (ms)',
        'min_response_time': 'Tempo M√≠nimo (ms)',
        'median_response_time': 'Tempo Mediano (ms)',
        'avg_requests_per_sec': 'Req/s M√©dio',
        'total_requests': 'Total Requisi√ß√µes',
        'failure_count': 'Total Falhas',
        'success_rate': 'Taxa Sucesso (%)',
        'repetitions': 'Repeti√ß√µes',
        'p95_response_time': 'P95 (ms)',
        'p99_response_time': 'P99 (ms)'
    }
    
    df_comparison = df_comparison.rename(columns=column_mapping)
    
    # Adicionar informa√ß√µes sobre os cen√°rios
    scenario_labels = ['Cen√°rio A (50 usu√°rios)', 'Cen√°rio B (100 usu√°rios)', 'Cen√°rio C (200 usu√°rios)']
    df_comparison.index = pd.Index(scenario_labels)
    
    # Formatar n√∫meros
    for col in ['Tempo M√©dio (ms)', 'Tempo M√°ximo (ms)', 'Tempo M√≠nimo (ms)', 'Tempo Mediano (ms)', 'P95 (ms)', 'P99 (ms)']:
        if col in df_comparison.columns:
            df_comparison[col] = df_comparison[col].round(1)
    
    for col in ['Req/s M√©dio']:
        if col in df_comparison.columns:
            df_comparison[col] = df_comparison[col].round(1)
    
    for col in ['Taxa Sucesso (%)']:
        if col in df_comparison.columns:
            df_comparison[col] = df_comparison[col].round(2)
    
    return df_comparison

def create_charts(scenario_stats, output_dir="analysis"):
    """Cria gr√°ficos comparativos"""
    os.makedirs(output_dir, exist_ok=True)
    
    # Configurar estilo
    plt.style.use('default')
    
    # Dados para os gr√°ficos
    scenarios = ['Cen√°rio A\n(50 usu√°rios)', 'Cen√°rio B\n(100 usu√°rios)', 'Cen√°rio C\n(200 usu√°rios)']
    users = [50, 100, 200]  # Usu√°rios por cen√°rio
    
    # Gr√°fico 1: Compara√ß√£o de Performance
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('An√°lise de Performance - Spring PetClinic Microservices', fontsize=16, fontweight='bold')
    
    # 1. Tempo de Resposta M√©dio
    avg_times = [scenario_stats['CenarioA']['avg_response_time'], 
                 scenario_stats['CenarioB']['avg_response_time'], 
                 scenario_stats['CenarioC']['avg_response_time']]
    
    bars1 = ax1.bar(scenarios, avg_times, color=['#2E86AB', '#A23B72', '#F18F01'], alpha=0.8)
    ax1.set_title('Tempo M√©dio de Resposta por Cen√°rio', fontsize=14, fontweight='bold')
    ax1.set_ylabel('Tempo (ms)', fontsize=12)
    ax1.grid(True, alpha=0.3)
    
    # Adicionar valores nos barras
    for bar, value in zip(bars1, avg_times):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_times)*0.01, 
                f'{value:.1f}ms', ha='center', va='bottom', fontweight='bold')
    
    # 2. Throughput (Req/s)
    throughput = [scenario_stats['CenarioA']['avg_requests_per_sec'], 
                  scenario_stats['CenarioB']['avg_requests_per_sec'], 
                  scenario_stats['CenarioC']['avg_requests_per_sec']]
    
    bars2 = ax2.bar(scenarios, throughput, color=['#2E86AB', '#A23B72', '#F18F01'], alpha=0.8)
    ax2.set_title('Throughput por Cen√°rio', fontsize=14, fontweight='bold')
    ax2.set_ylabel('Requisi√ß√µes/segundo', fontsize=12)
    ax2.grid(True, alpha=0.3)
    
    # Adicionar valores nos barras
    for bar, value in zip(bars2, throughput):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(throughput)*0.01, 
                f'{value:.1f} req/s', ha='center', va='bottom', fontweight='bold')
    
    # 3. Taxa de Sucesso
    success_rates = [scenario_stats['CenarioA']['success_rate'], 
                     scenario_stats['CenarioB']['success_rate'], 
                     scenario_stats['CenarioC']['success_rate']]
    
    bars3 = ax3.bar(scenarios, success_rates, color=['#2E86AB', '#A23B72', '#F18F01'], alpha=0.8)
    ax3.set_title('Taxa de Sucesso por Cen√°rio', fontsize=14, fontweight='bold')
    ax3.set_ylabel('Taxa de Sucesso (%)', fontsize=12)
    ax3.set_ylim(95, 100.5)
    ax3.grid(True, alpha=0.3)
    
    # Adicionar valores nos barras
    for bar, value in zip(bars3, success_rates):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, 
                f'{value:.2f}%', ha='center', va='bottom', fontweight='bold')
    
    # 4. Escalabilidade (Usu√°rios vs Throughput)
    ax4.plot(users, throughput, marker='o', linewidth=3, markersize=10, color='#2E86AB')
    ax4.set_title('Escalabilidade: Usu√°rios vs Throughput', fontsize=14, fontweight='bold')
    ax4.set_xlabel('N√∫mero de Usu√°rios Simult√¢neos', fontsize=12)
    ax4.set_ylabel('Throughput (req/s)', fontsize=12)
    ax4.grid(True, alpha=0.3)
    
    # Adicionar linha ideal de escalabilidade
    ideal_throughput = [throughput[0] * (u/users[0]) for u in users]
    ax4.plot(users, ideal_throughput, '--', color='gray', alpha=0.7, label='Escalabilidade Ideal')
    ax4.legend()
    
    # Adicionar valores nos pontos
    for x, y in zip(users, throughput):
        ax4.annotate(f'{y:.1f}', (x, y), textcoords="offset points", xytext=(0,10), ha='center', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/performance_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Gr√°fico adicional: Percentis de Lat√™ncia
    fig2, ax = plt.subplots(1, 1, figsize=(12, 8))
    
    p95_times = [scenario_stats['CenarioA']['p95_response_time'], 
                 scenario_stats['CenarioB']['p95_response_time'], 
                 scenario_stats['CenarioC']['p95_response_time']]
    p99_times = [scenario_stats['CenarioA']['p99_response_time'], 
                 scenario_stats['CenarioB']['p99_response_time'], 
                 scenario_stats['CenarioC']['p99_response_time']]
    
    x = range(len(scenarios))
    width = 0.35
    
    bars1 = ax.bar([i - width/2 for i in x], avg_times, width, label='Tempo M√©dio', color='#2E86AB', alpha=0.8)
    bars2 = ax.bar([i + width/2 for i in x], p95_times, width, label='P95', color='#A23B72', alpha=0.8)
    
    ax.set_title('Compara√ß√£o de Lat√™ncias por Cen√°rio', fontsize=16, fontweight='bold')
    ax.set_ylabel('Tempo de Resposta (ms)', fontsize=12)
    ax.set_xlabel('Cen√°rios', fontsize=12)
    ax.set_xticks(x)
    ax.set_xticklabels(scenarios)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Adicionar valores
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2, height + max(p95_times)*0.01,
                   f'{height:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=10)
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/latency_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Gr√°ficos salvos em {output_dir}/")
    print(f"  - performance_comparison.png")
    print(f"  - latency_comparison.png")

def generate_report(scenario_stats, comparison_table, output_dir="analysis"):
    """Gera relat√≥rio final em texto"""
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report = f"""
RELAT√ìRIO DE PERFORMANCE - SPRING PETCLINIC MICROSERVICES
=========================================================

Data/Hora: {timestamp}

RESUMO DOS CEN√ÅRIOS TESTADOS:
-----------------------------
‚Ä¢ Cen√°rio A: 50 usu√°rios simult√¢neos por 10 minutos
‚Ä¢ Cen√°rio B: 100 usu√°rios simult√¢neos por 10 minutos  
‚Ä¢ Cen√°rio C: 200 usu√°rios simult√¢neos por 5 minutos

Mix de endpoints testados:
‚Ä¢ GET /api/customer/owners (40% das requisi√ß√µes)
‚Ä¢ GET /api/customer/owners/{{id}} (30% das requisi√ß√µes)
‚Ä¢ GET /api/vet/vets (20% das requisi√ß√µes)
‚Ä¢ POST /api/customer/owners (10% das requisi√ß√µes)

RESULTADOS CONSOLIDADOS:
-----------------------
{comparison_table.to_string()}

AN√ÅLISE COMPARATIVA:
-------------------

1. TEMPO DE RESPOSTA:
"""
    
    # An√°lise detalhada
    cenario_a = scenario_stats['CenarioA']
    cenario_b = scenario_stats['CenarioB'] 
    cenario_c = scenario_stats['CenarioC']
    
    report += f"""
   ‚Ä¢ Cen√°rio A (50 usu√°rios): {cenario_a['avg_response_time']:.1f}ms em m√©dia (P95: {cenario_a['p95_response_time']:.1f}ms)
   ‚Ä¢ Cen√°rio B (100 usu√°rios): {cenario_b['avg_response_time']:.1f}ms em m√©dia (P95: {cenario_b['p95_response_time']:.1f}ms)
   ‚Ä¢ Cen√°rio C (200 usu√°rios): {cenario_c['avg_response_time']:.1f}ms em m√©dia (P95: {cenario_c['p95_response_time']:.1f}ms)
   
   üîç QUANDO DOBRAMOS OS USU√ÅRIOS (50‚Üí100): O tempo m√©dio {"aumentou" if cenario_b['avg_response_time'] > cenario_a['avg_response_time'] else "diminuiu"} {abs(cenario_b['avg_response_time'] - cenario_a['avg_response_time']):.1f}ms ({((cenario_b['avg_response_time'] - cenario_a['avg_response_time'])/cenario_a['avg_response_time']*100):+.1f}%).
   üîç NO PICO (200 usu√°rios): O tempo m√©dio foi {cenario_c['avg_response_time'] - cenario_a['avg_response_time']:+.1f}ms comparado ao cen√°rio base ({((cenario_c['avg_response_time'] - cenario_a['avg_response_time'])/cenario_a['avg_response_time']*100):+.1f}%).

2. THROUGHPUT (REQUISI√á√ïES POR SEGUNDO):
   ‚Ä¢ Cen√°rio A: {cenario_a['avg_requests_per_sec']:.1f} req/s
   ‚Ä¢ Cen√°rio B: {cenario_b['avg_requests_per_sec']:.1f} req/s ({((cenario_b['avg_requests_per_sec'] - cenario_a['avg_requests_per_sec'])/cenario_a['avg_requests_per_sec']*100):+.1f}%)
   ‚Ä¢ Cen√°rio C: {cenario_c['avg_requests_per_sec']:.1f} req/s ({((cenario_c['avg_requests_per_sec'] - cenario_a['avg_requests_per_sec'])/cenario_a['avg_requests_per_sec']*100):+.1f}%)
   
   üîç ESCALABILIDADE: {"O sistema escala bem" if cenario_c['avg_requests_per_sec'] > cenario_b['avg_requests_per_sec'] > cenario_a['avg_requests_per_sec'] else "O sistema n√£o escala linearmente"} - cada duplica√ß√£o de usu√°rios resultou em {"aumento proporcional" if cenario_c['avg_requests_per_sec']/cenario_a['avg_requests_per_sec'] > 3 else "aumento menor que o esperado"} no throughput.

3. CONFIABILIDADE:
   ‚Ä¢ Taxa de sucesso Cen√°rio A: {cenario_a['success_rate']:.2f}% ({cenario_a['failure_count']} falhas)
   ‚Ä¢ Taxa de sucesso Cen√°rio B: {cenario_b['success_rate']:.2f}% ({cenario_b['failure_count']} falhas)
   ‚Ä¢ Taxa de sucesso Cen√°rio C: {cenario_c['success_rate']:.2f}% ({cenario_c['failure_count']} falhas)
   
   üîç NO PICO: A taxa de sucesso {"se manteve est√°vel" if cenario_c['success_rate'] > 99 else f"caiu {cenario_a['success_rate'] - cenario_c['success_rate']:.2f} pontos percentuais"} quando chegamos a 200 usu√°rios.

CONCLUS√ïES:
----------
{generate_conclusions(scenario_stats)}

RECOMENDA√á√ïES:
-------------
{generate_recommendations(scenario_stats)}
"""
    
    # Salvar relat√≥rio
    report_file = f"{output_dir}/relatorio_performance.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"Relat√≥rio salvo em {report_file}")
    return report

def generate_conclusions(scenario_stats):
    """Gera conclus√µes autom√°ticas baseadas nos dados"""
    cenario_a = scenario_stats['CenarioA']
    cenario_b = scenario_stats['CenarioB'] 
    cenario_c = scenario_stats['CenarioC']
    
    conclusions = []
    
    # An√°lise de escalabilidade
    if cenario_c['avg_requests_per_sec'] > cenario_a['avg_requests_per_sec'] * 1.5:
        conclusions.append("‚Ä¢ O sistema demonstra boa escalabilidade, aumentando o throughput proporcionalmente aos usu√°rios.")
    else:
        conclusions.append("‚Ä¢ O sistema apresenta limita√ß√µes de escalabilidade, n√£o conseguindo manter o throughput proporcional ao aumento de usu√°rios.")
    
    # An√°lise de lat√™ncia  
    latency_increase = ((cenario_c['avg_response_time'] - cenario_a['avg_response_time']) / cenario_a['avg_response_time']) * 100
    if latency_increase < 50:
        conclusions.append("‚Ä¢ O tempo de resposta se mant√©m est√°vel mesmo com aumento significativo de carga.")
    else:
        conclusions.append(f"‚Ä¢ Houve degrada√ß√£o significativa no tempo de resposta ({latency_increase:.1f}% de aumento) sob alta carga.")
    
    # An√°lise de confiabilidade
    min_success = min(cenario_a['success_rate'], cenario_b['success_rate'], cenario_c['success_rate'])
    if min_success > 99:
        conclusions.append("‚Ä¢ O sistema mant√©m excelente confiabilidade (>99% sucesso) em todos os cen√°rios.")
    elif min_success > 95:
        conclusions.append("‚Ä¢ O sistema mant√©m boa confiabilidade (>95% sucesso) mesmo sob carga.")
    else:
        conclusions.append("‚Ä¢ A confiabilidade do sistema √© comprometida sob alta carga.")
    
    return "\n".join(conclusions)

def generate_recommendations(scenario_stats):
    """Gera recomenda√ß√µes baseadas nos resultados"""
    cenario_c = scenario_stats['CenarioC']
    
    recommendations = []
    
    if cenario_c['avg_response_time'] > 1000:
        recommendations.append("‚Ä¢ Considerar otimiza√ß√£o de consultas ao banco de dados e cache.")
    
    if cenario_c['success_rate'] < 99:
        recommendations.append("‚Ä¢ Implementar circuit breakers e retry policies mais robustos.")
        
    if cenario_c['avg_requests_per_sec'] < 100:
        recommendations.append("‚Ä¢ Avaliar configura√ß√£o de pool de conex√µes e recursos de CPU/mem√≥ria.")
    
    recommendations.append("‚Ä¢ Realizar testes com carga sustentada por per√≠odos mais longos.")
    recommendations.append("‚Ä¢ Monitorar m√©tricas de infraestrutura (CPU, mem√≥ria, I/O) durante os testes.")
    
    return "\n".join(recommendations)

def main():
    """Fun√ß√£o principal"""
    print("Iniciando an√°lise dos resultados de performance...")
    
    # Carregar dados
    results = load_results_data()
    
    # Calcular estat√≠sticas por cen√°rio
    scenario_stats = {}
    for scenario, dfs in results.items():
        stats = calculate_scenario_stats(dfs)
        if stats:
            scenario_stats[scenario] = stats
            print(f"Processado {scenario}: {stats['repetitions']} repeti√ß√µes")
    
    if not scenario_stats:
        print("Nenhum resultado encontrado! Certifique-se de que os testes foram executados.")
        return
    
    # Criar tabela comparativa
    comparison_table = create_comparison_table(scenario_stats)
    print("\nTabela Comparativa:")
    print(comparison_table)
    
    # Criar gr√°ficos
    create_charts(scenario_stats)
    
    # Gerar relat√≥rio
    report = generate_report(scenario_stats, comparison_table)
    
    print("\nAn√°lise conclu√≠da! Arquivos gerados na pasta 'analysis'")

if __name__ == "__main__":
    main()